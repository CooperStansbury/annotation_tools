{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goals\n",
    "\n",
    "1. Ingest and 'clean' annotations from multiple json files, annotators, and informed consent documents\n",
    "1. Ingest and 'clean' raw textual content from informed consent documents (including documents corrupted by upstream processes)\n",
    "1. Align annotations against their respective informed consent documents\n",
    "1. Save cannonical files for next analysis steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import hashlib\n",
    "import spacy\n",
    "import re\n",
    "import random\n",
    "from importlib import reload\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Get a list of files from the `data/` directory.\n",
    "Each is a `.json. file with annotations for multiple informed consent documents. \n",
    "Each `.json` file is a single annotator.\n",
    "\"\"\"\n",
    "\n",
    "class Raw_Annotations():\n",
    "    \"\"\"A class to help manage annotations from DataTurks\"\"\"\n",
    "    \n",
    "    def _get_date(self):\n",
    "        \"\"\" return  today's date as an appendable string\"\"\"\n",
    "        return datetime.today().strftime(\"%m-%d-%Y\")\n",
    "    \n",
    "    def __init__(self, data_dir = \"../data/\", nlp_lib=\"en_core_web_lg\"):\n",
    "        \"\"\"\n",
    "        Args: \n",
    "            - data_dir (str): a path to a directory of json files \n",
    "                containing annotations        \n",
    "        \"\"\"\n",
    "        self.nlp = spacy.load(nlp_lib)\n",
    "        self.data_dir = data_dir\n",
    "        self.annotated_files = self.get_json_files()\n",
    "        self.annotators = self.get_annotators()\n",
    "\n",
    "        # load annotations into a list. \n",
    "        # THIS IS THE WORKHORSE OF THIS CLASS\n",
    "        self.annotations = self.load_annotations()\n",
    "        \n",
    "        # map all document raw content to it's ID\n",
    "        self.document_map = self.build_document_map()\n",
    "        \n",
    "        \n",
    "    def format_annotator_name(self, filename):\n",
    "        \"\"\"A function to return the formatted name of an annotator given a consistently \n",
    "        named file.\n",
    "\n",
    "        Note: this depends on files named like `_NAME.json`\n",
    "\n",
    "        Args:\n",
    "            - filename (str): a file name, expects name after last `_`\n",
    "                character\n",
    "\n",
    "        Returns:\n",
    "            - name (str): a formated string \n",
    "        \"\"\"\n",
    "        return filename.split(\"_\")[-1].split(\".\")[0].upper()\n",
    "\n",
    "    def get_json_files(self):\n",
    "        \"\"\"A function to initialize a dictionary for storing annotations \n",
    "\n",
    "        Returns:\n",
    "            - annotated_files (default dict): primary keys are annotators names\n",
    "        \"\"\"\n",
    "        annotated_files = []\n",
    "\n",
    "        for subdir, dirs, files in os.walk(self.data_dir):\n",
    "                for file in files:\n",
    "                    annotated_files.append(os.path.join(subdir, file))\n",
    "\n",
    "        return annotated_files\n",
    "    \n",
    "    def get_annotators(self):\n",
    "        \"\"\"A function to get the names of annotators\"\"\"\n",
    "        annotators = []\n",
    "        for file in self.annotated_files:\n",
    "            name = self.format_annotator_name(file)\n",
    "            if name not in annotators:\n",
    "                annotators.append(name)\n",
    "                \n",
    "        return annotators\n",
    "    \n",
    "    def get_document_id(self, raw_content, digits=8):\n",
    "        \"\"\"A function to facilitate conversion of raw text into document ids\n",
    "\n",
    "        Args:\n",
    "             - raw_content (str): a sufficient portion of the document as to \n",
    "                 be unique with a high probability\n",
    "             - digits (int): number of digits to return\n",
    "\n",
    "         Returns:\n",
    "             - doc_id (int): a document id\n",
    "        \"\"\"\n",
    "        return int(hashlib.sha256(raw_content.encode('utf-8')).hexdigest(), 16) % 10**digits\n",
    "    \n",
    "    def build_document_map(self):\n",
    "        \"\"\" A function to get a map of documents and ids \"\"\"\n",
    "        \n",
    "        document_map = {}\n",
    "        \n",
    "        for json_file in self.annotated_files:\n",
    "             for annotated_doc in open(json_file):\n",
    "                json_dump = json.loads(annotated_doc)\n",
    "                # get raw content\n",
    "                content = json_dump['content']\n",
    "                doc_id = self.get_document_id(content)\n",
    "                document_map[doc_id] = {'raw_content': content,\n",
    "                                        'from_file':json_file}\n",
    "                \n",
    "        return document_map\n",
    "    \n",
    "    def get_dumps(self, json_file):\n",
    "        \"\"\"A function to return a list of json_dumps\n",
    "        from a given json_file\n",
    "        \n",
    "        Args:\n",
    "            - json_file (str): the path to the json file\n",
    "            \n",
    "        Returns:\n",
    "            - json_dumps (list): list of json dumps \n",
    "        \"\"\"\n",
    "        return [json.loads(dump) for dump in open(json_file)]\n",
    "    \n",
    "    \n",
    "    def _clean_raw_annotation_text(self, raw_text_annotation):\n",
    "        \"\"\"A function to clean sentences\n",
    "        \n",
    "        Args: \n",
    "            - raw_text_annotation (str): may be multi-sentence annotations\n",
    "            \n",
    "        Returns:\n",
    "            - clean_list (list): a list of clean sentences   \n",
    "        \"\"\"\n",
    "        dirty_str = str(raw_text_annotation).strip().encode(encoding = 'ascii',\n",
    "                                                       errors = 'replace')\n",
    "        dirty_str = dirty_str.decode(encoding='ascii', \n",
    "                           errors='strict')\n",
    "        \n",
    "        dirty_str = str(dirty_str).replace(\"?\", \" \")\n",
    "        # strip redundant whitespace and signature lines \n",
    "        dirty_str = re.sub(' +', ' ', dirty_str).replace(\"_\", \"\")\n",
    "        \n",
    "        clean_list = []\n",
    "        \n",
    "        for sent in self.nlp(\" \".join(dirty_str.split())).sents:\n",
    "            clean_list.append(sent.text)\n",
    "        \n",
    "        return clean_list\n",
    "    \n",
    "    def load_annotations(self):\n",
    "        \"\"\"A function to load all annotations into a list\n",
    "        \"\"\"\n",
    "        \n",
    "        processed_annotations = []\n",
    "        \n",
    "#         for json_file in random.sample(self.annotated_files, 1):\n",
    "        for json_file in self.annotated_files:\n",
    "            name = self.format_annotator_name(json_file)\n",
    "            \n",
    "            # each json file contains multiple\n",
    "            # informed consent forms\n",
    "            # iterate through consent docs in\n",
    "            # a json file\n",
    "            for icd_doc in self.get_dumps(json_file):\n",
    "                             \n",
    "                # handle None annotations\n",
    "                if icd_doc['annotation'] is None:\n",
    "                    continue \n",
    "                \n",
    "                # hash document content to create ID\n",
    "                doc_id = self.get_document_id(icd_doc['content'])\n",
    "                    \n",
    "                for annotation in icd_doc['annotation']:\n",
    "                    \n",
    "                    # perform preprocessing on sentences in annotation\n",
    "                    # primarily to handle multiple sentence annotations\n",
    "                    sentences = self._clean_raw_annotation_text(annotation['points'][0]['text'])\n",
    "                    \n",
    "                    # start and end character positions recorded\n",
    "                    # only for the first sentence, everything else\n",
    "                    # will require an offset during alignment\n",
    "                    start_char = annotation['points'][0]['start']\n",
    "                    end_char = annotation['points'][0]['end']\n",
    "                    \n",
    "                    # track the number of sentences\n",
    "                    # with the index\n",
    "                    for idx, sent in enumerate(sentences):\n",
    "                        \n",
    "                        processed_annotations.append({\n",
    "                            'ICD_doc_id' : doc_id,\n",
    "                            'json_filename':json_file,\n",
    "                            'annotator': name,\n",
    "                            'annotation_id': self.get_document_id(annotation['points'][0]['text'], 8),\n",
    "                            'A': 1 if 'A' in annotation['label'] else 0,\n",
    "                            'B': 1 if 'B' in annotation['label'] else 0,\n",
    "                            'C': 1 if 'C' in annotation['label'] else 0,\n",
    "                            'start_char':start_char,\n",
    "                            'end_char':end_char,\n",
    "                            'text':sent,\n",
    "                            'sentence_count': (idx + 1)\n",
    "                        })\n",
    "                        \n",
    "        return processed_annotations\n",
    "    \n",
    "            \n",
    "    def save_annotations(self, out_dir=\"../output/\"):\n",
    "        \"\"\"A function to save files\n",
    "        \n",
    "        Args:\n",
    "            - outdir (str): the directory path of the output\n",
    "        \"\"\"\n",
    "        filename = f\"{out_dir}ANNOTATIONS_{self._get_date()}.csv\"\n",
    "        df = pd.DataFrame(self.annotations)\n",
    "        df.to_csv(filename, index=False)\n",
    "        print(f\"Saved to: `{filename}`\")\n",
    "        \n",
    "    def save_document_map(self, out_dir=\"../output/\"):\n",
    "        \"\"\"A function to save the document map. \n",
    "        \n",
    "        Args:\n",
    "            - outdir (str): the directory path of the output\n",
    "        \"\"\"\n",
    "        filename = f\"{out_dir}DOCUMENT_MAP_{self._get_date()}.json\"\n",
    "        json.dump(self.document_map, open(filename, 'w'))\n",
    "        print(f\"Saved to: `{filename}`\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 µs, sys: 1 µs, total: 5 µs\n",
      "Wall time: 9.06 µs\n",
      "Saved to: `../output/ANNOTATIONS_02-12-2020.csv`\n",
      "Saved to: `../output/DOCUMENT_MAP_02-12-2020.json`\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "annotations = Raw_Annotations()\n",
    "annotations.save_annotations()\n",
    "annotations.save_document_map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO Manual Correction of forms\n",
    "# TODO Match annotations to reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
